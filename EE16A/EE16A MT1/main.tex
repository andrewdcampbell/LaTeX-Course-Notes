\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{empheq}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{gensymb}
\usepackage{multicol}
\setlength{\parskip}{0.5cm plus4mm minus3mm}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{upgreek}
\usepackage[nobreak=true]{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1.0in]{geometry}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{titlesec}
\usepackage{chngcntr}
\newcommand{\norm}[1]{\lvert #1 \rvert}

\begin{document}
\date{}
\title{\vspace{-5ex}Midterm 1 Cheat Sheet - EE16A Spring 2016\vspace{-5ex}}
\maketitle
% \begin{multicols}{2}
\begin{enumerate}
    \item \textbf{Vector space:} A vector space $V$ is a set of elements that is closed under vector addition and scalar multiplication. For $V$ to be a vector space, the following conditions must hold for every $\vec{u},\vec{v},\vec{z} \in$ $V$ and for every $c,d$ $\in \mathbb{R}$.
    \begin{enumerate}
        \item No escape property (addition): $\vec{u}+\vec{v} \in$ $V$.
        \item No escape property (scalar multiplication): $c\vec{u} \in$ $V$.
        \item Commutativity: $\vec{u}+\vec{v}=\vec{v}+\vec{u}$.
        \item Associativity of vector addition: $(\vec{u}+\vec{v})+\vec{z} =\vec{u}+(\vec{v}+\vec{z})$.
        \item Additive identity: There is $\vec{0} \in$ $V$ such that for all $\vec{u}$, $\vec{0}+\vec{u}=\vec{u}$.
        \item Existence of inverse: For every $\vec{u}$, there is element $-\vec{u}$ such that $\vec{u}+(-\vec{u}) = \vec{0}$.
        \item Associativity of scalar multiplication: $c(d(\vec{u})) = (cd)\vec{u}$.
        \item Distributivity of scalar sums: $(c+d)\vec{u} = c\vec{u}+d\vec{u}$.
        \item Distributivity of vector sums: $c(\vec{u}+\vec{v}) = \vec{u}+\vec{v}$.
        \item Scalar multiplication identity: There is $1\vec{u} =\vec{u}$.
    \end{enumerate}
    
    \item \textbf{Subspace:}: A subspace of $\mathbb{R}^n$ is any set $H$ in $\mathbb{R}^n$ for which three properties apply: 
    \begin{enumerate}
        \item The zero vector $\vec{0}$ is in $H$.
        \item For each $\vec{u}$ and $\vec{v}$ in $H$, the sum $\vec{u}+\vec{v}$ is in $H$.
        \item for each $\vec{u}$ in $H$, the vector $c\vec{u}$ is in $H$.
    \end{enumerate}
    
    \item \textbf{Linear (In)dependence:} A set of vectors $\left\{\vec{v}_1,\vec{v_2},\ldots,\vec{v}_p\right\}$ in $\mathbb{R}^n$ is said to be \textbf{linearly independent} if the equation $c_1\vec{v_1} + c_2\vec{v_2} + \ldots + c_p\vec{v_p} = 0$ has only the trivial solution. The set is said to be \textbf{linearly dependent} if there exist weights $c_1, c_2,\ldots, c_p$, not all zero, such that $c_1\vec{v_1} + c_2\vec{v_2} + \ldots + c_p\vec{v_p} = 0$.

    \item \textbf{Basis:} A set of vectors $\beta$ $\left\{\vec{b}_1,\vec{b_2},\ldots,\vec{b}_p\right\}$ in $V$ is a \textbf{basis} for $H$ if $\beta$ is a linearly independent set, and the subspace spanned by $\beta$ coincides with $H$, that is, $H = Span\left\{\vec{b}_1,\vec{b_2},\ldots,\vec{b}_p\right\}$.
    
    \item \textbf{Basis Theorem:} Let $H$ be a p-dimensional subspace of $\mathbb{R}^n$. Any linearly independent set of exactly p elements in $H$ is automatically a basis for $H$. Also any set of p elements of $H$ that spans $H$ is automatically a basis for $H$.
    
    \item \textbf{Important subspaces:}
    \begin{enumerate}
    
    \item The \textbf{column space} of a matrix $A$ is the set \textit{Col} $A$ of all linear combinations of the columns of $A$. The column space of an $m$ x $n$ matrix is a subspace of $\mathbb{R}^m$. 
    
    \item The \textbf{row space} of a matrix $A$ is the set of all linear combinations of the rows of $A$. 
    
    \item The \textbf{null space} of a matrix $A$ is the set \textit{Nul} $A$ of all solutions to the homogeneous equation $A\vec{x} = \vec{0}$. The null space of an $m$ x $n$ matrix is a subspace of $\mathbb{R}^n$.
    
    \item The \textbf{left nullspace} of $A$ is \textit{Nul} $A^T$.
    \end{enumerate}
    \item \textbf{Inverse:} An $n$ x $n$ matrix $A$ is said to be invertible if there is an $n$ x $n$ matrix $B$ such that $BA = I_n = AB$.
    
    \item \textbf{Dimension} of a subspace $H$, denoted by dim $H$, is the number of vectors in any basis for $H$.
    
    \item \textbf{Span} $\left\{\vec{v}_1,\ldots,\vec{v}_p\right\}$ is the collection of all vectors that can be written in the form $c_1\vec{v}_1 + c_2\vec{v}_2 + \ldots + c_p\vec{v}_p$ with $c_1, c_2, \ldots, c_p$ scalars.
    
    \item \textbf{Dynamic Systems/Transition matrices}: Each row tells the proportion of input it will draw from the particular reservoirs at the next time step. 
    
    Each column tells where its output will end up in the other reservoirs at the next time step. The columns of the transition matrix can easily be constructed then by seeing what happens to the unit vectors. If the columns sum to 1, the system has no loss or gain.
    
    \item \textbf{Vector algebra} \\
    The dot product of vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ is defined as:
    \begin{equation*}
       \vec{u}\cdot\vec{v} = \begin{bmatrix} u_1 \\ \vdots \\ u_n \end{bmatrix}
       \cdot \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}
       = u_1v_1+\ldots u_nv_n
    \end{equation*}
    The length of a vector is defined as:
    \begin{equation*}
        \norm{\norm{\vec{u}}}=\sqrt{\vec{u}\cdot\vec{u}}
    \end{equation*}
    For vector lengths:
    \begin{equation*}
        \vec{u}\cdot\vec{v}=\norm{\norm{\vec{u}}}\hspace{.5mm}\norm{\norm{\vec{v}}}\cos{\theta}
    \end{equation*}
    
    \item \textbf{Rotation matrix}
    \begin{equation*}
        \begin{bmatrix} 
        \cos{\theta} & -\sin{\theta} \\
        \sin{\theta} & \cos{\theta}
        \end{bmatrix}
    \end{equation*}
    
    \item \textbf{Example proof:}
    If \{$\vec{v_1},\vec{v_2}, \hdots, \vec{v_k}$\} is a linearly dependent set of vectors, then there exist scalars \{$c_{1},c_{2}, \hdots, c_{k}$\}, not all zero, such that we have
    \begin{equation}
        c_{1}\vec{v_1}+c_{2}\vec{v_2}+\hdots+c_{k}\vec{v_k}=0
    \end{equation}
    In order to prove linear dependence of \{$A\vec{v_1},A\vec{v_2}, \hdots, A\vec{v_k}$\}, we must show 
    \begin{equation}
        c_{1}(A\vec{v_1})+c_{2}(A\vec{v_2})+\hdots+c_{k}(A\vec{v_k})=0
    \end{equation}
    for some scalars \{$c_{1},c_{2}, \hdots, c_{k}$\}, not all zero.
    We left-multiply this equation by A to get
    \begin{equation}
        A\left(
        c_{1}\vec{v_1}+c_{2}\vec{v_2}+\hdots+c_{k}\vec{v_k}
        \right)=A0=0
    \end{equation}
    By the distributive property of matrix multiplication, which states $A(B+C)=AB+AC$, and the commutative property of scalar multiplication, which states $Ac$\hspace{1 mm}=\hspace{1 mm}$cA$, we arrive at
    \begin{equation}
        c_{1}(A\vec{v_1})+c_{2}(A\vec{v_2})+\hdots+c_{k}(A\vec{v_k})=0
    \end{equation}
    Therefore, the same $k$ scalars \{$c_{1},c_{2}, \hdots, c_{k}$\} show the linear dependence of the vectors \{$A\vec{v_1},A\vec{v_2}, \hdots, A\vec{v_k}$\}.
\end{enumerate}
% \end{multicols}
\end{document}
